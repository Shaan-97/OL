{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6f2f83",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2347032910.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [2]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Q1 8v\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Q1 8v\n",
    "Big Data Is An Evolving Term That Describes Any Voluminous Amount Of Structured, Semi-structured\n",
    "And Unstructured Data That Has The Potential To Be Mined For Information\n",
    "▪ Big Data Originally Was Characterized By 3vs:\n",
    "• The Extreme Volume Of Data\n",
    "• The Wide Variety Of Types Of Data\n",
    "• The Velocity At Which The Data Is Generated\n",
    "▪ Then We Talked Of 5vs:\n",
    "• Veracity Or Uncertainty Of Data\n",
    "• The Value Of Data\n",
    "▪ Today We Talk Of 8vs:\n",
    "• Visualization To Make Sense Of Data At A Glance\n",
    "• Viscosity Would You Want To Keep The Data With You / Something Useful Or Important\n",
    "• Virality Is There A Chance That The Data May Go Viral? Can The Data Be Used Further In A Post Etc?\n",
    " \n",
    "Hadoop\n",
    "an open-source framework of software\n",
    "▪ for storage & large-scale processing\n",
    "▪ of massive data-sets\n",
    "▪ on clusters\n",
    "▪ of commodity hardware\n",
    "Storage -> Hadoop Distributed File System\n",
    "▪ Process -> Map Reduce Paradigm\n",
    "▪ Analyze -> Hive, Pig, Impala, etc\n",
    " \n",
    "Hadoop can scale to multiple nodes (1,500–2,000) in a cluster \n",
    "▪ Just configuration changes are required\n",
    " It is not OLAP (online analytical processing) but batch / offline oriented\n",
    "▪ It is not a database\n",
    "▪ Predictive Analysis\n",
    "▪ Sentiment Analysis\n",
    "▪ Customer Intelligence\n",
    "▪ Fraud & Security Intelligence\n",
    "▪ High-Performance Analytics \n",
    "▪ Risk Managemen\n",
    "\n",
    "HDFS ARCHITECHRTURE\n",
    "Not Designed For \n",
    "▪ Small files. \n",
    "▪ Multiple writes, arbitrary file modification \n",
    "▪ Writes are always supported at the end of the file\n",
    "▪ Modifications can't be made at random offsets of files\n",
    "▪ Low latency data access\n",
    "▪ Since we are accessing huge amount of data it comes at the expense of time taken to access data\n",
    "HDFS Design\n",
    "▪ Individual files are broken into blocks of fixed size (typically 256 MB blocks)\n",
    "▪ Stored across cluster of nodes (not necessarily on same machine)\n",
    "▪ These files can be more than the size of individual machine's hard drive\n",
    "▪ So access to a file requires cooperation of several nodes\n",
    "Design Challenges\n",
    "▪ System expects large files for processing ; small number of very large files would be stored\n",
    "▪ Several machines involved in storing a file, loss of machine should be handled\n",
    "Name Node\n",
    "▪ Name Node is controller and manager of HDFS\n",
    "▪ It knows the status and the metadata of all the files in HDFS\n",
    "▪ Metadata [ file names, permissions, and locations of each block of file ]\n",
    "▪ HDFS cluster can be accessed concurrently by multiple clients, even then this metadata information \n",
    "is never desynchronized; hence, all this information is handled by a single machine\n",
    "▪ Since metadata is typically small, all this info is stored in main memory of Name Node, allowing fast \n",
    "access to metadata\n",
    "Data Node\n",
    "▪ Actual data is stored on the Data Node\n",
    "▪ Knows only about the data stored on it\n",
    "▪ Will read data and send to client when retrieval requested\n",
    "▪ Will receive data and store locally when storage is requested\n",
    "Name Node HA\n",
    "▪ Function of name node is very critical for overall health of HDFS\n",
    "▪ If individual data nodes fail, HDFS can recover and function with a little less capacity \n",
    "▪ Crash of name node can lose all the information and the complete file system irrecoverably\n",
    "▪ That's why metadata and involvement of name node in data transfer is kept minimal\n",
    "▪ Name Node can also be set to work in HA\n",
    "Secondary Name Node\n",
    "▪ It is not backup of name node nor data nodes connect to this; it is just a helper of name node.\n",
    "▪ It only performs periodic checkpoints\n",
    "▪ It communicates with name node and to take snapshots of HDFS metadata\n",
    "▪ These snapshots help minimize downtime and loss of data\n",
    "\n",
    "HDFS Read\n",
    "Info to client to read a file, it goes to Distributed file system, then it goes to name node and gets block \n",
    "location depending upon metadata.  \tReturn to client and gives data to fs data input stream. Goes to resp data\n",
    "nodes and read a file. After process over, it closes the fs system.\n",
    "\n",
    "HDFS write\n",
    "INFO TO CLIENT TO WRITE A FILE, IT GOES TO DISTRIBUTED FILE SYSTEM. IT CONNECTS WITH NAME NODE TO KNOW THE LOCATION \n",
    "DAT NODES INTERACTS WITH NAME NODE AND GIVES HEART SGNALS EVERY 3 SEC, WHERE STORAGE IS EMPTY AND WHERE IT CAN BE STORED. \n",
    "NAME NODE GETS LOCATION ND SENDS TO CLIENT \tCLIENT GIVES TO FS DATA OUTPUT STREAM. IT CONNECTS WITH DATA NODE AND WRITE. \n",
    "CLOSE AFTER PROCESS OVER.\n",
    "\n",
    "HDFS Features\n",
    "▪ Data is distributed over several machines\n",
    "▪ Replicated to ensure their durability to failure & high availability to parallel applications\n",
    "▪ Designed for very large files (in GBs, TBs)\n",
    "▪ Block oriented\n",
    "▪ Write once and read many time\n",
    "\n",
    "Processing Issues - Traditional\n",
    "▪ Data is processed sequentially. \n",
    "▪ There are millions of records which get processed and hence it takes hours \n",
    "▪ Takes more time as the number of records increases\n",
    "▪ Can run out of memory\n",
    "\n",
    "Processing Issues - HDFS\n",
    "▪ Documents spread multiple disks\n",
    "▪ Process document-part on each disk and provide aggregated solution\n",
    "▪ How to divide the data in equal parts\n",
    "▪ Combining results will take further processing\n",
    "\n",
    "What Is Map-Reduce\n",
    "▪ Split input files (e.g., by HDFS blocks)\n",
    "▪ Move code to data \n",
    "▪ Operate on key / value pairs\n",
    "▪ Mappers filter and transform input data\n",
    "▪ Sort & Shuffle provides order to mapper data\n",
    "▪ Reducers aggregate the mapper output (post Sort & Shuffle)\n",
    "▪ Stores Reducer output in HDFS\n",
    "\n",
    "Map-Reduce Steps\n",
    "▪ Split input data in independent chunks is already available via HDFS\n",
    "▪ Job needs to be scheduled to carry out required process\n",
    "▪ Schedule tasks on nodes where data is already present\n",
    "▪ Map Phase – Transformation Phase\n",
    "=> input Data | output – list <key, value> pairs\n",
    "▪ Sort & Shuffle – Group & Order Phase\n",
    "=> input – list of <key , value> pairs | output – sorted & grouped list of <key, value> pairs\n",
    "▪ Reduce Phase – Aggregation Phase\n",
    "=> sorted & grouped list of <key , value> pairs | output – aggregated <key, value>\n",
    "\n",
    "MRV1\n",
    "▪ The Client\n",
    "▪ The Job Tracker\n",
    "▪ The Task Tracker\n",
    "▪ MR Tasks\n",
    "▪ HDFS\n",
    "▪ A Client invokes a Map-Reduce, from a Calling Node (maybe a Data Node or an Extra Node in the cluster)\n",
    "▪ An instance of Job Tracker is created in the memory of the Calling Node.\n",
    "▪ The Job Tracker queries the Name Node and finds the Data Nodes (location of the data to be used). \n",
    "▪ Job Tracker then launches Task Trackers in the memory of all the Data Nodes as above to run the jobs \n",
    "▪ Job Tracker gives the code to Task Tracker to run as a Task\n",
    "▪ Task Tracker is responsible for creating the tasks & running the tasks\n",
    "▪ In effect the Mapper of the Job is found here\n",
    "▪ Once the Task is completed, the result from the Tasks is sent back to the Job Tracker\n",
    "▪ Job Tracker also keeps a track of progress by each Task Tracker\n",
    "▪ The Job Tracker also receives the results from each Task Tracker and aggregates the results\n",
    "▪ In effect the Reducer of the Job is found here\n",
    "\n",
    "MRV2\n",
    "▪ The Client\n",
    "▪ Resource Manager\n",
    "▪ App Node Manager\n",
    "▪ Application Master\n",
    "▪ Task Node Manager\n",
    "▪ MR Task\n",
    "▪ HDFS \n",
    "▪ A Client invokes a Map-Reduce, from a Calling Node (maybe a Data Node or an Extra Node in the cluster)\n",
    "▪ An instance of Resource Manager is created in the memory of the Calling Node.\n",
    "▪ The Resource Manager then launches containers with appropriate resources (memory) with App Node \n",
    "Manager in memory of the Calling Node \n",
    "▪ Along with this Application Master is invoked. Application Master is “pause” mode till all containers with \n",
    "Task Node Manager (as below) are created.\n",
    "▪ The Resource Manager queries the Name Node and finds the Data Nodes (location of the data used). \n",
    "▪ The Resource Manager then launches containers with appropriate resources (memory) with Task Node \n",
    "Manager in all the Data Nodes as above to run the jobs \n",
    "▪ Application Master gives the code to Task Node Manager to run as a Task\n",
    "▪ Task Node Manager is responsible for creating & running tasks. In effect the Mapper of the Job is here\n",
    "▪ Once the Task is completed, the result from the Tasks is sent back to the Application Master\n",
    "▪ Application Master also keeps a track of progress by each Task Node Manager\n",
    "▪ The Application Master lso receives the results from each Task Node Manager and aggregates the results\n",
    "▪ In effect the Reducer of the Job is found here\n",
    "▪ Thus from previous version, Job Tracker has been replaced by Resource Manager & Application Master\n",
    "▪ From previous version, TaskTracker has been replaced by Task Node Managers \n",
    "\n",
    "Map-Reduce Failure Recovery\n",
    "MRv1\n",
    "▪ Task Failure – new task is started by the Task Tracker\n",
    "▪ Task Tracker Failure – new Task Tracker is started by the Job Tracker\n",
    "▪ Job Tracker Failure – no recovery; single point of failure\n",
    "MRv2\n",
    "▪ Task Failure \n",
    "–– new task is started by Task Node Manager\n",
    "▪ Task Node Manager Failure \n",
    "–– new container with Task Node Manager is created by Resource Manager \n",
    "–– this Task Node Manager is given the code and started by Application Master\n",
    "▪ Application Master Failure \n",
    "–– new Application Master is started by App Node Manager\n",
    "▪ App Node Manager Failure \n",
    "–– new container with App Node Manager is created by Resource Manager \n",
    "–– this App Node Manager invokes the Application Master\n",
    "▪ Resource Manager Failure \n",
    "–– new resource manager with saved state is started\n",
    "\n",
    "\n",
    "Columnar Database\n",
    "A columnar database, also known as a column oriented database\n",
    "A DBMS that stores data in columns rather than in rows as RDBMS\n",
    "The table schema defines only column families, which are the key value pairs.\n",
    "A table can have multiple column families and each column family can have any number of\n",
    "columns.\n",
    "Subsequent column values are stored contiguously on the disk.\n",
    "Each cell value of the table has a timestamp.\n",
    "The tables in Columnar database are sorted by row.\n",
    "The rows of tables in Columnar database are identified by a row key.\n",
    "The difference centered around performance, storage and schema modifying techniques.\n",
    "Efficient write & read data to and from hard disk storage; speeds up the time it takes to return a\n",
    "query.\n",
    "Main benefits columnar operations like MIN, MAX, SUM, COUNT and AVG performed very\n",
    "rapidly.\n",
    "Columnar Database allows random read & write in Hadoop.\n",
    "Columnar database fulfills the ACID principles of a database.\n",
    "\n",
    "\n",
    "Apache Pig Distributed Programming\n",
    "Pig provides an engine for executing data flows in parallel on Hadoop\n",
    "It includes a language, Pig Latin, for expressing these data flows\n",
    "Pig Latin includes operators for many of the traditional data operations (join, sort, filter, etc.),\n",
    "as well as the ability for users to develop their own functions for reading, processing, and\n",
    "writing data\n",
    "Pig runs on Hadoop\n",
    "It makes use of both the Hadoop Distributed File System, HDFS, and Hadoop’s processing\n",
    "system, Map Reduce\n",
    "3 Components:\n",
    "Pig Latin Interpreter:\n",
    "The Pig Latin Interpreter is the first component of the Pig architecture. It takes the Pig Latin script written by the user and translates it into a logical plan. The logical plan consists of a series of MapReduce operations that will be executed on the data.\n",
    "Pig Latin Compiler:\n",
    "The Pig Latin Compiler is the second component of the Pig architecture. It takes the logical plan generated by the interpreter and translates it into a physical plan. The physical plan consists of a series of MapReduce jobs that will be executed on the Hadoop cluster.\n",
    "Pig Latin Execution Environment:\n",
    "The Pig Latin Execution Environment is the final component of the Pig architecture. It is responsible for executing the MapReduce jobs generated by the Pig Latin Compiler. The execution environment creates a series of MapReduce jobs based on the physical plan generated by the compiler. It then submits these jobs to the Hadoop cluster for execution. The results of the MapReduce jobs are then returned to the user.\n",
    "\n",
    "\n",
    "Apache Spark Distributed Programming\n",
    "Spark is used for data analytics in cluster computing framework\n",
    "Spark fits into the Hadoop open source community, building on top of the Hadoop Distributed\n",
    "File System (HDFS)\n",
    "Spark provides an easier to use alternative to Hadoop Map Reduce and offers performance up to 10 times faster than previous generation systems like Hadoop Map Reduce for certain applications\n",
    "Spark is a framework for writing fast, distributed programs\n",
    "Spark solves similar problems as Hadoop Map Reduce does but with a fast in memory approach.\n",
    "Spark can be used interactively to quickly process and query big data sets\n",
    "Spark has inbuilt tools for interactive query analysis (Shark)\n",
    "Large scale graph processing and analysis (Bagel)•\n",
    "Real time analysis (Spark Streaming)\n",
    "Spark basically uses an in memory MapReduce approach\n",
    "Spark revolves around the concept of a Resilient Distributed Dataset (RDD).\n",
    "RRD is a faulttolerant collection of elements that can be operated on in parallel.\n",
    "There are two ways to create RDDs:\n",
    " •parallelizing an existing collection in your driver program,\n",
    "•referencing a dataset in a shared filesystem like HDFS or a DBMS \n",
    "\n",
    "HADOOP VS SPARK\n",
    "Hadoop- job tracker; spark – spark driver\n",
    "Task tracker = spark worker (both works on data node) (same architecture)\n",
    "Differences between Hadoop and Spark:\n",
    "Processing Model: Hadoop uses a MapReduce model, which is a batch processing model that processes data in a two-step process: map and reduce. On the other hand, Spark uses an in-memory processing model that allows for data to be processed much faster than Hadoop by keeping the data in memory.\n",
    "Speed: Spark is generally faster than Hadoop because it uses in-memory processing, which allows for faster access to data. Hadoop, on the other hand, reads data from disk, which is slower than accessing data from memory.\n",
    "Ease of Use: Spark has a more user-friendly API than Hadoop, which makes it easier for developers to use. Hadoop requires developers to write more code to process data, while Spark provides a more streamlined API.\n",
    "Libraries: Spark provides a wide range of libraries for processing data, including Spark SQL, Spark Streaming, and MLlib for machine learning. Hadoop provides fewer libraries, and developers may need to use third-party libraries to achieve similar functionality.\n",
    "Similarities between Hadoop and Spark:\n",
    "Data Storage: Both Hadoop and Spark use HDFS for storing data. HDFS is a distributed file system that provides fault tolerance and high availability for storing large volumes of data.\n",
    "Distributed Computing: Both Hadoop and Spark are designed for distributed computing, which allows them to process large volumes of data by distributing processing across multiple nodes in a cluster.\n",
    "Cluster Management: Both Hadoop and Spark require a cluster management system, such as YARN or Mesos, for managing resources and scheduling jobs.\n",
    "In summary, both Hadoop and Spark are designed for distributed computing and use HDFS for storing data. However, Spark is generally faster than Hadoop due to its in-memory processing model and provides a more user-friendly API and a wider range of libraries for processing data.\n",
    "\n",
    "Apache Hive Architecture\n",
    "Apache Hive is a data warehousing tool that allows users to perform SQL-like queries on large datasets stored in Hadoop distributed file systems (HDFS) or other compatible file systems. Hive translates SQL-like queries into MapReduce jobs that can be executed on a Hadoop cluster. The architecture of Apache Hive can be divided into three main components:\n",
    "Hive Metastore: The Hive Metastore is a central repository that stores metadata about the data stored in Hive. It includes information about tables, partitions, columns, and their data types, storage location, and serialization formats. The metastore is responsible for managing the schema of the data and providing metadata to the Hive Query Engine for query optimization.\n",
    "Hive Query Engine: The Hive Query Engine is responsible for translating SQL-like queries into MapReduce jobs that can be executed on a Hadoop cluster. It includes several modules, such as Compiler, Optimizer, and Execution Engine. The Compiler translates SQL queries into logical and physical plans that can be optimized by the Optimizer. The Execution Engine executes the optimized plan by generating MapReduce jobs that can be executed on the Hadoop cluster.\n",
    "Hadoop Distributed File System (HDFS): HDFS is a distributed file system that is used to store the data managed by Hive. Hive stores the data in HDFS in the form of files and partitions. HDFS provides fault-tolerance and scalability by replicating the data across multiple nodes in a Hadoop cluster.\n",
    "When a user submits a query to Hive, the Hive Query Engine receives the query and generates a logical and physical plan for the query. The logical plan represents the operations required to execute the query, and the physical plan represents the steps required to execute the operations in a MapReduce job. The plan is optimized by the optimizer to generate a more efficient plan. The optimized plan is then passed to the Execution Engine, which generates the MapReduce jobs to execute the query on the Hadoop cluster.\n",
    "During query execution, the Execution Engine communicates with the JobTracker, which is responsible for scheduling and monitoring the execution of MapReduce jobs on the Hadoop cluster. The results of the MapReduce job are then returned to the user as the result of the query.\n",
    "In summary, Apache Hive architecture consists of three main components: Hive Metastore, Hive Query Engine, and Hadoop Distributed File System (HDFS). Hive Metastore manages the metadata of the data stored in Hive, the Hive Query Engine translates SQL queries into MapReduce jobs, and HDFS stores the data managed by Hive. Together, these components provide a scalable and fault-tolerant data warehousing solution for big data processing.\n",
    "\n",
    "\n",
    "CODING\n",
    "HDFS COMMANDS\n",
    "Copy line 22- mkdir and put- hdfs commands\n",
    "Load,foreach[line 37], dump,store using pig storage(customer)(line66)\n",
    "Dropping table and database\n",
    "Show databases\n",
    "Use hr\n",
    "Drop table employee\n",
    "Drop database hr\n",
    "Create database hr and table and load from pig\n",
    "\n",
    "\n",
    "CODING FOR SPLIT AND JOIN\n",
    "grunt> income = load '/myhdfs/BDA/input/IncomeLevel.txt' as (line:chararray)\n",
    "A = foreach income generate (int) SUBSTRING (line,0,2) as Age, (chararray) \n",
    "SUBSTRING (line,3,19) as WorkClass, (int) SUBSTRING (line,20,26) as Fnlwgt, \n",
    "(chararray) SUBSTRING (line,28,40) as Education,(chararray) SUBSTRING (line,41,43) \n",
    "as EducationLevel,(chararray) SUBSTRING (line,44,65) as MaritalClass, (chararray) \n",
    "SUBSTRING (line,66,83) as Occupation, (chararray) SUBSTRING (line,84,98) as \n",
    "Relationship, (chararray) SUBSTRING (line,99,117) as Race, (chararray) SUBSTRING \n",
    "(line,118,124) as Sex, (int) SUBSTRING (line,125,130) as CapitalGain, (int) \n",
    "SUBSTRING (line,131,135) as CapitalLoss, (int) SUBSTRING (line,136,138) as \n",
    "HoursPerWeek, (chararray) SUBSTRING (line,139,165) as NativeCountry, (chararray) \n",
    "SUBSTRING (line,166,172) as IncomeLevel\n",
    "\n",
    "store A into '/myhdfs/BDA/Output' using PigStorage(','\n",
    "\n",
    "Spliting\n",
    "students = load '/myhdfs/BDA/Output/Income_level.csv' using PigStorage(',') as (id:int, firstname:chararray, lastname:chararray, age:int, phone:chararray, city:chararray); \n",
    "SPLIT students into students1 if age<23, students2 if (age>=23);\n",
    "dump students1;\n",
    "dump students2;\n",
    "store students1 into '/myhdfs/BDA/Output1/s1' using PigStorage(','\n",
    "store students1 into '/myhdfs/BDA/Output1/s2' using PigStorage(','\n",
    "\n",
    "HIVE\n",
    "After tables are created\n",
    "Create table ss1\n",
    "Create table ss2\n",
    "LOAD DATA inpath 'hdfs:///myhdfs/BDA/Output1/s1' OVERWRITE INTO \n",
    "TABLE ss1\n",
    "LOAD DATA inpath 'hdfs:///myhdfs/BDA/Output1/s2' OVERWRITE INTO \n",
    "TABLE ss2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545116f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
